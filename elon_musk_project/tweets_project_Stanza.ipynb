{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path for all tweet files to concatenate them\n",
    "path = r'./elon_musk_tweets'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    # print(filename)\n",
    "    frame = pd.read_csv(filename, index_col=None, encoding=\"utf-8\")\n",
    "    li.append(frame)\n",
    "\n",
    "# print(li)\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "df.shape\n",
    "# df[35000:35101]\n",
    "# df.head(10)\n",
    "# df.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon = df[[\"date\",\"tweet\"]]\n",
    "# df_elon.date[:50]\n",
    "# df_elon[:10]\n",
    "# df_elon.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving tweets to a list\n",
    "dates = df_elon[\"date\"].to_list()\n",
    "# print(dates[-1])\n",
    "tweets = df_elon[\"tweet\"].to_list()\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en') # To comment out after first usage when the model is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER with Stanza from StanfordNLP to see it in action\n",
    "# def stanza_nlp(text):\n",
    "#   nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "#   doc = nlp(text)\n",
    "#   print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n",
    "  # print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')\n",
    "# stanza_nlp(tweets[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORG extraction with Stanza\n",
    "def mentioned_orgs(tweets):\n",
    "    \"\"\"Function takes in a list of tweets and returns a dictionary with organisation's name as a key and a list of tweets and dates of posting as a value\"\"\"\n",
    "    comps = {}\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "    count = 0\n",
    "    for tweet in tweets:\n",
    "        doc = nlp(tweet)\n",
    "        for sent in doc.sentences:\n",
    "            for ent in sent.ents:\n",
    "                if ent.type == \"ORG\":\n",
    "                    if ent.text not in comps:\n",
    "                        comps[ent.text] = list()\n",
    "                        comps[ent.text].append(list())\n",
    "                        comps[ent.text][0].append(dates[count])\n",
    "                        comps[ent.text][0].append(tweets[count])\n",
    "                        # comps[ent.text] = 1\n",
    "                    else:\n",
    "                        comps[ent.text].append(list())\n",
    "                        comps[ent.text][-1].append(dates[count])\n",
    "                        comps[ent.text][-1].append(tweets[count])\n",
    "                        # comps[ent.text] += 1\n",
    "        count += 1\n",
    "    \n",
    "    return comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = mentioned_orgs(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times each company is mentioned\n",
    "top_comps = {}\n",
    "for k in companies:\n",
    "    top_comps[k] = len(companies[k])\n",
    "    # print(k, len(comps[k]))\n",
    "    \n",
    "print(top_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting dictionary in descending order\n",
    "top_comps = sorted(top_comps.items(), key=lambda x: x[1], reverse=True)\n",
    "print(top_comps[:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_products = {\n",
    "    # \"Model S\": \"Tesla\"\n",
    "# }\n",
    "\n",
    "comp_products = {\n",
    "    \"Zip2\": \"Zip2\",\n",
    "    \"Falcon\": \"SpaceX\",\n",
    "    \"Falcon Heavy\": \"SpaceX\",\n",
    "    \"Falcon 9\": \"SpaceX\",\n",
    "    \"Grasshopper\": \"SpaceX\", \n",
    "    \"Starship\": \"SpaceX\", \n",
    "    \"Merlin\": \"SpaceX\", \n",
    "    \"Kestrel\": \"SpaceX\", \n",
    "    \"Raptor\": \"SpaceX\", \n",
    "    \"Methox thruster\": \"SpaceX\", \n",
    "    \"Draco\": \"SpaceX\", \n",
    "    \"SuperDraco\": \"SpaceX\", \n",
    "    \"Dragon\": \"SpaceX\", \n",
    "    \"Cargo Dragon\": \"SpaceX\", \n",
    "    \"Starlink\": \"SpaceX\", \n",
    "    \"Hyperloop\": \"SpaceX\", \n",
    "    \"vactrain\": \"SpaceX\",\n",
    "    \"Model S\": \"Tesla\",\n",
    "    \"Roadster\": \"Tesla\",\n",
    "    \"Model X\": \"Tesla\", \n",
    "    \"Model 3\": \"Tesla\", \n",
    "    \"Model Y\": \"Tesla\", \n",
    "    \"Semi\": \"Tesla\", \n",
    "    \"Cybertruck\": \"Tesla\", \n",
    "    \"Powerwall\": \"Tesla\", \n",
    "    \"Solar Roof\": \"Tesla\", \n",
    "    \"Solar Inverter\": \"Tesla\", \n",
    "    \"Powerpack\": \"Tesla\", \n",
    "    \"Megapack\": \"Tesla\", \n",
    "    \"Supercharger\": \"Tesla\", \n",
    "    \"Destination charger\": \"Tesla\", \n",
    "    \"Autopilot\": \"Tesla\", \n",
    "    \"Full self-driving\": \"Tesla\", \n",
    "    \"FSD\": \"Tesla\",\n",
    "    \"Application-Specific Integrated Circuit\": \"Neuralink\",\n",
    "    \"ASIC\": \"Neuralink\", \n",
    "    \"brain-computer interface\": \"Neuralink\",\n",
    "    \"Hawthorne tunnel\": \"The Boring Company\", \n",
    "    \"Los Angeles tunnel\": \"The Boring Company\", \n",
    "    \"Loop\": \"The Boring Company\", \n",
    "    \"Loop System\": \"The Boring Company\",\n",
    "    \"Gym\": \"OpenAI\", \n",
    "    \"RoboSumo\": \"OpenAI\", \n",
    "    \"Debate Game\": \"OpenAI\", \n",
    "    \"Dactyl\": \"OpenAI\", \n",
    "    \"GPT\": \"OpenAI\", \n",
    "    \"generative pre-training\": \"OpenAI\", \n",
    "    \"GPT-2\": \"OpenAI\", \n",
    "    \"GPT-3\": \"OpenAI\", \n",
    "    \"MuseNet\": \"OpenAI\", \n",
    "    \"API\": \"OpenAI\", \n",
    "    \"DALL-E\": \"OpenAI\", \n",
    "    \"CLIP\": \"OpenAI\", \n",
    "    \"Microscope\": \"OpenAI\", \n",
    "    \"Codex\": \"OpenAI\", \n",
    "    \"Five\": \"OpenAI\", \n",
    "    \"GYM Retro\": \"OpenAI\"\n",
    "}\n",
    "\n",
    "# print(comp_products[\"Model S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data processed with Spacy checking the entities against the names of companies' products.\n",
    "new_d = {}\n",
    "for k in companies:\n",
    "    if k in comp_products:\n",
    "        # print(k)\n",
    "        # print(comp_products[k])\n",
    "        # print(companies[k])\n",
    "        if comp_products[k] not in new_d:\n",
    "            new_d[comp_products[k]] = companies[k]\n",
    "        else:\n",
    "            new_d[comp_products[k]].extend(companies[k])\n",
    "    else:\n",
    "        if k in new_d:\n",
    "            # print(k)\n",
    "            new_d[k].extend(companies[k])\n",
    "        else:\n",
    "            # print(k)\n",
    "            new_d[k] = companies[k]\n",
    "\n",
    "# print(len(new_d[\"Tesla\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to count times each company was mentioned\n",
    "new_d_count = {}\n",
    "for k in new_d:\n",
    "    new_d_count[k] = len(new_d[k])\n",
    "    # print(k, len(elon_companies[k]))\n",
    "    \n",
    "# print(new_d_count)\n",
    "\n",
    "new_d_sorted = sorted(new_d_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Companies Elon Musk most talks about:\\n\")\n",
    "count = 1\n",
    "for k,v in new_d_sorted[:31]:\n",
    "    print(\"{}. {} ({} tweets)\".format(count, k, v))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_tweets_to_list(companies_dict, company_name):\n",
    "    \"\"\"Function takes in a dictionary with tweets about companies and a string with the name of a company under consideration\"\"\"\n",
    "    comp_tweets = []\n",
    "\n",
    "    idx = 0\n",
    "    while idx in range(len(companies_dict[company_name])):\n",
    "        comp_tweets.append(companies_dict[company_name][idx][1])\n",
    "        idx += 1\n",
    "\n",
    "    return comp_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_tweets = company_tweets_to_list(new_d, company_name=\"Tesla\")\n",
    "# tesla_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacex_tweets = company_tweets_to_list(new_d, company_name=\"SpaceX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbc_tweets = company_tweets_to_list(new_d, company_name=\"The Boring Company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(lst_texts):\n",
    "    \"\"\"Function takes in a list of texts and returns a dictionary with keys as numbers and text and sentiment scores with explanation for a value\"\"\"\n",
    "    data = {}\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    count = 0\n",
    "    for text in lst_texts:\n",
    "        vs = analyzer.polarity_scores(text)\n",
    "        data[count] = list()\n",
    "        data[count].extend([text, vs])\n",
    "        data[count].append(data[count][1]['compound'])\n",
    "        if data[count][1]['compound'] > 0:\n",
    "            data[count].append(\"POSITIVE\")\n",
    "        elif data[count][1]['compound'] == 0:\n",
    "            data[count].append(\"NEUTRAL\")\n",
    "        else:\n",
    "            data[count].append(\"NEGATIVE\")\n",
    "        count += 1\n",
    "    # print(data[3000])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla = sentiment_analyzer(tesla_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_df = pd.DataFrame.from_dict(tesla, orient=\"index\",columns=[\"Tweet\",\"Scores\", \"Compound\", \"Sentiment_type\"])\n",
    "tesla_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_df.Sentiment_type.value_counts().plot(kind='bar',title=\"Tesla Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacex = sentiment_analyzer(spacex_tweets)\n",
    "spacex_df = pd.DataFrame.from_dict(spacex, orient=\"index\",columns=[\"Tweet\",\"Scores\", \"Compound\", \"Sentiment_type\"])\n",
    "spacex_df\n",
    "spacex_df.Sentiment_type.value_counts().plot(kind='bar',title=\"SpaceX Sentiment Analysis\", color=\"#334CFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbc = sentiment_analyzer(tbc_tweets)\n",
    "tbc_df = pd.DataFrame.from_dict(tbc, orient=\"index\",columns=[\"Tweet\",\"Scores\", \"Compound\", \"Sentiment_type\"])\n",
    "tbc_df\n",
    "tbc_df.Sentiment_type.value_counts().plot(kind='bar',title=\"Sentiment Analysis for The Boring Company\",color=\"#FF338A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_tweets = company_tweets_to_list(new_d, company_name=\"Apple\")\n",
    "# apple_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = sentiment_analyzer(apple_tweets)\n",
    "apple_df = pd.DataFrame.from_dict(apple, orient=\"index\",columns=[\"Tweet\",\"Scores\", \"Compound\", \"Sentiment_type\"])\n",
    "apple_df\n",
    "apple_df.Sentiment_type.value_counts().plot(kind='bar',title=\"Apple Sentiment Analysis\",color=\"#35950E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_tweets = company_tweets_to_list(new_d, company_name=\"Google\")\n",
    "google = sentiment_analyzer(google_tweets)\n",
    "google_df = pd.DataFrame.from_dict(google, orient=\"index\",columns=[\"Tweet\",\"Scores\", \"Compound\", \"Sentiment_type\"])\n",
    "google_df\n",
    "google_df.Sentiment_type.value_counts().plot(kind='bar',title=\"Google Sentiment Analysis\",color=\"#DE3A20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_tweets = company_tweets_to_list(new_d, company_name=\"Twitter\")\n",
    "twitter = sentiment_analyzer(twitter_tweets)\n",
    "twitter_df = pd.DataFrame.from_dict(twitter, orient=\"index\",columns=[\"Tweet\",\"Scores\", \"Compound\", \"Sentiment_type\"])\n",
    "twitter_df\n",
    "twitter_df.Sentiment_type.value_counts().plot(kind='bar',title=\"Twitter Sentiment Analysis\",color=\"#51CEE5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
